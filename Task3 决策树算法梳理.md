# Task3 决策树算法梳理
##### 信息论基础
1.熵：本是热力学中表征物质状态的参量之一，用符号S表示，其物理意义是体系混乱程度的度量。对于机器学习算法来说，熵指代香农熵，是一种不确定性度量。它是表示随机变量不确定的度量，是对所有可能发生的事件产生的信息量的期望。对于事件X，有n种可能结果，且概率分别为p1，p2，…，pn，则熵H(X)为：

基本性质：
均匀分布具有最大的熵。一个好的不确定性度量会在均匀分布时达到最大的值。给定n个可能的结果，在所有结果的概率相同时得到最大的熵。
对于独立事件，熵是可加的。两个独立事件的联合熵等于各个独立事件的熵的和。
具有非零概率的结果数量增加，熵也会增加。加入发生概率为0的结果并不会有影响。
连续性。不确定性度量应该是连续的，熵函数是连续的。
具有更多可能结果的均匀分布有更大的不确定性。
非负性。事件拥有非负的不确定性。
确定事件的熵为0。有确定结果的事件具有0不确定性。
参数排列不变性。调转参数顺序没有影响。
2.联合熵：一维随机变量分布推广到多维随机变量分布，则其联合熵 (Joint entropy) 为：

注：熵只依赖于随机变量的分布，与随机变量取值无关。
3.条件熵： H(Y|X) 表示在已知随机变量 X 的条件下随机变量 Y 的不确定性。条件熵 H(Y|X) 定义为 X 给定条件下 Y 的条件概率分布的熵对  X 的数学期望。

条件熵 H(Y|X) 相当于联合熵 H(X,Y) 减去单独的熵 H(X)，即H(Y|X)=H(X,Y)−H(X)。证明：

当已知 H(X) 这个信息量的时候，H(X,Y) 剩下的信息量就是条件熵，描述 X 和 Y 所需的信息是描述 X 自己所需的信息,加上给定  X 的条件下具体化  Y 所需的额外信息。
4.信息增益：以某特征划分数据集前后的熵的差值。即待分类集合的熵和选定某个特征的条件熵之差（这里只的是经验熵或经验条件熵，由于真正的熵并不知道，是根据样本计算出来的），公式如下：

注：这里不要理解偏差，因为上边说了熵是类别的，但是在这里又说是集合的熵，没区别，因为在计算熵的时候是根据各个类别对应的值求期望来等到熵
5.基尼不纯度：将来自集合中的某种结果随机应用于集合中某一数据项的预期误差率。即从一个数据集中随机选取子项，度量其被错误的划分到其他组里的概率。

（1）显然基尼不纯度越小，纯度越高，集合的有序程度越高，分类的效果越好；
（2）基尼不纯度为 0 时，表示集合类别一致；
（3）基尼不纯度最高（纯度最低）时，

例，如果集合中的每个数据项都属于同一分类，此时误差率为 0。如果有四种可能的结果均匀地分布在集合中，此时的误差率为 1−0.25=0.75；


###### 决策树的不同分类算法（ID3算法、C4.5、CART分类树）的原理及应用场景
  * ID3算法 ID3决策树可以有多个分支，但是不能处理特征值为连续的情况。决策树是一种贪心算法，每次选取的分割数据的特征都是当前的最佳选择，并不关心是否达到最优
   *  C4.5 ID3的基础上改进而提出的
  *   CART分类树 改进了前两种算法中的一个缺点：使用信息增益或信息增益比时，可选值多的特征往往有更高的信息增益

###### 回归树原理
回归是为了处理预测值是连续分布的情景，其返回值应该是一个具体预测值。回归树的叶子是一个个具体的值，从预测值连续这个意义上严格来说，回归树不能称之为“回归算法”。因为回归树返回的是“一团”数据的均值，而不是具体的、连续的预测值（即训练数据的标签值虽然是连续的，但回归树的预测值却只能是离散的）。所以回归树其实也可以算为“分类”算法，其适用场景要具备“物以类聚”的特点，即特征值的组合会使标签属于某一个“群落”，群落之间会有相对鲜明的“鸿沟”。利用回归树可以将复杂的训练数据划分成一个个相对简单的群落，群落上可以再利用别的机器学习模型再学习。
###### 决策树防止过拟合手段
产生过拟合数据的原因：
原因1：样本问题
（1）样本里的噪音数据干扰过大，大到模型过分记住了噪音特征，反而忽略了真实的输入输出间的关系；（什么是噪音数据？）

（2）样本抽取错误，包括（但不限于）样本数量太少，抽样方法错误，抽样时没有足够正确考虑业务场景或业务特点，等等导致抽出的样本数据不能有效足够代表业务逻辑或业务场景；

（3）建模时使用了样本中太多无关的输入变量。
原因2：构建决策树的方法问题

在决策树模型搭建中，我们使用的算法对于决策树的生长没有合理的限制和修剪的话，决策树的自由生长有可能每片叶子里只包含单纯的事件数据或非事件数据，可以想象，这种决策树当然可以完美匹配（拟合）训练数据，但是一旦应用到新的业务真实数据时，效果是一塌糊涂。

上面的原因都是现象，但是其本质只有一个，那就是“业务逻辑理解错误造成的”，无论是抽样，还是噪音，还是决策树等等，如果我们对于业务背景和业务知识非常了解，非常透彻的话，一定是可以避免绝大多数过拟合现象产生的。因为在模型从确定需求，到思路讨论，到搭建，到业务应用验证，各个环节都是可以用业务敏感来防止过拟合于未然的。
针对原因1的解决方法：

合理、有效地抽样，用相对能够反映业务逻辑的训练集去产生决策树；

针对原因2的解决方法（主要）：

剪枝：提前停止树的增长或者对已经生成的树按照一定的规则进行后剪枝。
######  模型评估
平均均方误差mse
$MSE=1n∑(y¯−yi)2=Var(Y)MSE = \frac{1}{n}\sum(\bar{y}-y_i)^2 = Var(Y）$
拟合优度Goodness of fit
拟合优度（Goodness of Fit）是指回归直线对观测值的拟合程度。度量拟合优度的统计量是可决系数（亦称确定系数）R^2.
最大值为1。$R^2$
的值越接近1，说明回归直线对观测值的拟合程度越好；反之，$R^2$的值越小，说明回归直线对观测值的拟合程度越差。

##### sklearn参数详解，Python绘制决策树
class sklearn.tree.DecisionTreeClassifier(criterion='gini',splitter='best',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features=None,random_state=None, max_leaf_nodes=None,class_weight=None, presort=False)
    * criterion:string类型，可选（默认为"gini"）     衡量分类的质量。支持的标准有"gini"代表的是Gini impurity(不纯度)与"entropy"代表的是information gain（信息增益）。
   *  splitter:string类型，可选（默认为"best"）     一种用来在节点中选择分类的策略。支持的策略有"best"，选择最好的分类，"random"选择最好的随机分类。
    * max_features:int,float,string or None 可选（默认为None）     在进行分类时需要考虑的特征数。     1.如果是int，在每次分类是都要考虑max_features个特征。     2.如果是float,那么max_features是一个百分率并且分类时需要考虑的特征数是int(max_features*n_features,其中n_features是训练完成时发特征数)。     3.如果是auto,max_features=sqrt(n_features)     4.如果是sqrt,max_features=sqrt(n_features)     5.如果是log2,max_features=log2(n_features)     6.如果是None，max_features=n_features     注意：至少找到一个样本点有效的被分类时，搜索分类才会停止。
   *  max_depth:int or None,可选（默认为"None"）     表示树的最大深度。如果是"None",则节点会一直扩展直到所有的叶子都是纯的或者所有的叶子节点都包含少于min_samples_split个样本点。忽视max_leaf_nodes是不是为None。
   *  min_samples_split:int,float,可选（默认为2）     区分一个内部节点需要的最少的样本数。         1.如果是int，将其最为最小的样本数。     2.如果是float，min_samples_split是一个百分率并且ceil(min_samples_split*n_samples)是每个分类需要的样本数。ceil是取大于或等于指定表达式的最小整数。
    * min_samples_leaf:int,float,可选（默认为1）     一个叶节点所需要的最小样本数：     1.如果是int，则其为最小样本数     2.如果是float，则它是一个百分率并且ceil(min_samples_leaf*n_samples)是每个节点所需的样本数。
   * min_weight_fraction_leaf:float,可选（默认为0）     一个叶节点的输入样本所需要的最小的加权分数。
    *max_leaf_nodes:int,None 可选（默认为None）     在最优方法中使用max_leaf_nodes构建一个树。最好的节点是在杂质相对减少。如果是None则对叶节点的数目没有限制。如果不是None则不考虑max_depth.
   * class_weight:dict,list of dicts,"Banlanced" or None,可选（默认为None）     表示在表{class_label:weight}中的类的关联权值。如果没有指定，所有类的权值都为1。对于多输出问题，一列字典的顺序可以与一列y的次序相同。     "balanced"模型使用y的值去自动适应权值，并且是以输入数据中类的频率的反比例。如：n_samples/(n_classes*np.bincount(y))。     对于多输出，每列y的权值都会想乘。     如果sample_weight已经指定了，这些权值将于samples以合适的方法相乘。
    *random_state:int,RandomState instance or None     如果是int,random_state 是随机数字发生器的种子；如果是RandomState，random_state是随机数字发生器，如果是None，随机数字发生器是np.random使用的RandomState instance.
  *  persort:bool,可选（默认为False）     是否预分类数据以加速训练时最好分类的查找。在有大数据集的决策树中，如果设为true可能会减慢训练的过程。当使用一个小数据集或者一个深度受限的决策树中，可以减速训练的过程。

refer：https://blog.csdn.net/Eebaik/article/details/88169329 
